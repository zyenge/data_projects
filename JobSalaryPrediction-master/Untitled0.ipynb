{
 "metadata": {
  "name": "Untitled0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import data_io\n",
      "from features import FeatureMapper, SimpleTransform\n",
      "import numpy as np\n",
      "import pickle\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "features = [('FullDescription-Bag of Words', 'FullDescription', CountVectorizer.fit(max_features=20)), ('Title-Bag of Words', 'Title', CountVectorizer(max_features=20)),('LocationRaw-Bag of Words', 'LocationRaw', CountVectorizer(max_features=20)), ('LocationNormalized-Bag of Words', 'LocationNormalized', CountVectorizer(max_features=20))]\n",
      "combined = FeatureMapper(features)\n",
      "print combined\n",
      "\n",
      "def fit(self, X, y=None):\n",
      "        for feature_name, column_name, extractor in self.features:\n",
      "            extractor.fit(X[column_name], y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unbound method fit() must be called with CountVectorizer instance as first argument (got nothing instead)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-c3757c95fb8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FullDescription-Bag of Words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FullDescription'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Title-Bag of Words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LocationRaw-Bag of Words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LocationRaw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'LocationNormalized-Bag of Words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LocationNormalized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: unbound method fit() must be called with CountVectorizer instance as first argument (got nothing instead)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string_1=train['FullDescription'][2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cls=CountVectorizer(min_df=0.0)\n",
      "cls.transform(string_1)\n",
      "#print cls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Vocabulary wasn't fitted or is empty!",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-54-a1835633b318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \"\"\"\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary wasn't fitted or is empty!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;31m# raw_documents can be an iterable so we don't know its size in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Vocabulary wasn't fitted or is empty!"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import data_io\n",
      "train = data_io.get_train_df()\n",
      "valid = data_io.get_valid_df()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "Index([Id, Title, FullDescription, LocationRaw, LocationNormalized, ContractType, ContractTime, Company, Category, SalaryRaw, SalaryNormalized, SourceName], dtype=object)"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train['LocationRaw'][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "0              Dorking, Surrey, Surrey\n",
        "1          Glasgow, Scotland, Scotland\n",
        "2    Hampshire, South East, South East\n",
        "3       Surrey, South East, South East\n",
        "4       Surrey, South East, South East\n",
        "5      Dorking, Surrey, Surrey, Surrey\n",
        "6                    Aberdeen, Borders\n",
        "7       MANCHESTER, Greater Manchester\n",
        "8                LEEDS, West Yorkshire\n",
        "9                         Aberdeen, UK\n",
        "Name: LocationRaw"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string_1=train['FullDescription'][2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listtupe=[('y','h'),('x','hh')]\n",
      "for name1, name2 in listtupe:\n",
      "    print name1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "y\n",
        "x\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = [('FullDescription-Bag of Words', 'FullDescription', CountVectorizer(max_features=20)),\n",
      "                ('Title-Bag of Words', 'Title', CountVectorizer(max_features=20)),\n",
      "                ('LocationRaw-Bag of Words', 'LocationRaw', CountVectorizer(max_features=20)),\n",
      "                ('LocationNormalized-Bag of Words', 'LocationNormalized', CountVectorizer(max_features=20))]\n",
      "for feature_name, column_name, extractor in features:\n",
      "    print extractor.fit(train['FullDescription'][:2], y=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CountVectorizer(analyzer=word, binary=False, charset=utf-8,\n",
        "        charset_error=strict, dtype=<type 'long'>, input=content,\n",
        "        lowercase=True, max_df=1.0, max_features=20, max_n=None, min_df=2,\n",
        "        min_n=None, ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None,\n",
        "        vocabulary=None)\n",
        "CountVectorizer(analyzer=word, binary=False, charset=utf-8,\n",
        "        charset_error=strict, dtype=<type 'long'>, input=content,\n",
        "        lowercase=True, max_df=1.0, max_features=20, max_n=None, min_df=2,\n",
        "        min_n=None, ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None,\n",
        "        vocabulary=None)\n",
        "CountVectorizer(analyzer=word, binary=False, charset=utf-8,\n",
        "        charset_error=strict, dtype=<type 'long'>, input=content,\n",
        "        lowercase=True, max_df=1.0, max_features=20, max_n=None, min_df=2,\n",
        "        min_n=None, ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None,\n",
        "        vocabulary=None)\n",
        "CountVectorizer(analyzer=word, binary=False, charset=utf-8,\n",
        "        charset_error=strict, dtype=<type 'long'>, input=content,\n",
        "        lowercase=True, max_df=1.0, max_features=20, max_n=None, min_df=2,\n",
        "        min_n=None, ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None,\n",
        "        vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extractor.fit(train['LocationRaw'], y=None)\n",
      "fea=extractor.transform(train['LocationRaw'])\n",
      "print fea.toarray().shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(244768, 20)\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "def identity(x):\n",
      "    return x\n",
      "converters = { \"FullDescription\" : identity\n",
      "             , \"Title\": identity\n",
      "             , \"LocationRaw\": identity\n",
      "             , \"LocationNormalized\": identity\n",
      "             }\n",
      "train=pd.read_csv('/Users/zyenge/Documents/python_project/Kaggel/Train_rev1.csv', converters=converters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}